# Overview of Ethical Considerations

# Ethical Considerations for Organizers and Participants

### Participants
- **Idea Generation**:
  - Avoid plagiarism and ensure attribution.
- **Coding Assistance**:
  - Verify generated code for originality and bias.
- **Content Creation**:
  - Ensure factual accuracy and originality.
- **Collaboration**:
  - Communicate transparently about LLM usage.

### Organizers
- **Event Management**:
  - Ensure fairness and transparency in automation.
- **Participant Support**:
  - Train LLMs to provide respectful and accurate support.
- **Judging**:
  - Use LLM evaluations as supplementary tools, not replacements.

## Additional Ethical considerations
If competition is involved, does using an LLM constitute 'cheating'?
it is important for organizers to be clear on rules and expectations
* How can participation rules address the use of LLMs? (to even the playing fields for all participants?)
Add - Intellectual property
What are additional considerations and concerns regarding authorship of code/output that come from LLMs
sharing data with LLMs that is sensitive or you don't own - this becomes part of the training data (!!!!)


## 5 ethical considerations from papers

### Based on the papers below here are 5 ethical considerations:

1. **Bias in Outputs**
   LLMs may produce outputs that reflect inherent biases in their training data, which can unintentionally perpetuate stereotypes or inequalities in hackathon solutions
   [ar5iv](https://ar5iv.org/abs/2406.11400) [ar5iv](https://ar5iv.org/abs/2306.06283)

2. **Transparency in Usage**
   Hackathon teams should clearly communicate how LLMs are used, ensuring participants and stakeholders understand AI's role in generating ideas or solutions
   [ar5iv](https://ar5iv.org/abs/2405.14445) [ar5iv](https://ar5iv.org/abs/2306.06283)
   - 2a. Is there an element of "shaming" with admitting to LLM use? Is there a way to survey anonymously to avoid the double bind?
   - 2b. Organizers should provide guidance on LLM use, including ethical considerations. This serves as an opportunity for education around ethical use of LLMs.
   - 2c. The user is still responsible for outputs.

3. **Over-reliance on AI**
   Excessive dependence on LLMs can lead to reduced critical thinking or failure to scrutinize AI-generated outputs, especially when data accuracy is crucial
   [ar5iv](https://ar5iv.org/pdf/2405.14445) [ar5iv](https://ar5iv.org/pdf/2306.06283v3)

4. **Safety and Misinformation**
   LLMs can produce incorrect or misleading information if not properly guided, posing risks in sensitive fields like healthcare, education, or scientific research
   [ar5iv](https://ar5iv.org/abs/2406.11400) [ar5iv](https://ar5iv.org/pdf/2306.06283v3)

5. **Ethical Accessibility**
   LLM-based tools should be designed to ensure inclusivity and fairness, avoiding barriers for less technically proficient participants or marginalized groups
   [ar5iv](https://ar5iv.org/abs/2406.11400) [ar5iv](https://ar5iv.org/pdf/2306.06283v3)
   - 5a. Groups with paid subscriptions might have access to better models and have an advantage.


### 5 Papers on arXiv That Have “LLMs” and “Hackathon” in the Abstract:

- **arXiv:2408.08878** [pdf, other]
  *Knowledge Prompting: How Knowledge Engineers Use Large Language Models*
  Authors: Elisavet Koutsiana et al.
  Submitted: 2 August, 2024; originally announced: August 2024.

- **arXiv:2406.11400** [pdf, other]
  *Large Language Models and Knowledge Graphs for Astronomical Entity Disambiguation*
  Authors: Golnaz Shapurian
  Submitted: 17 June, 2024; originally announced: June 2024.

- **arXiv:2405.14445** [pdf]
  *Exploring the Use of a Large Language Model for Data Extraction in Systematic Reviews: A Rapid Feasibility Study*
  Authors: Lena Schmidt et al.
  Abstract: This paper describes a rapid feasibility study of using GPT-4, a large...
  Journal Ref: Proceedings of the 3rd Workshop on Augmented Intelligence for Technology-Assisted Reviews Systems, 2024.

- **arXiv:2404.18479** [pdf]
  *ChatGPT as an Inventor: Eliciting the Strengths and Weaknesses of Current Large Language Models Against Humans in Engineering Design*
  Authors: Daniel Nygård Ege et al.
  Submitted: 29 April, 2024; originally announced: April 2024.

- **arXiv:2306.06283** [pdf, other]
  *14 Examples of How LLMs Can Transform Materials Science and Chemistry: A Reflection on a Large Language Model Hackathon*
  Authors: Kevin Maik Jablonka et al.
  Submitted: 14 July, 2023; v1 submitted: 9 June, 2023; originally announced: June 2023.
