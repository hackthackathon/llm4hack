
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLMs in Hackathons</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&family=Raleway:ital@0;1&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="llm4hack_styles.css" >
</head>
<body>
    <div id="lmm4hack_page">
     
<nav id="llm4hack_nav">
    <ul>
        <li><a href="index.html">Home</a></li>
<li><a href="llms_for_hackathons.html">LLMs for hackathons</a></li>
<li><a href="organizers.html">For organizers</a></li>
<li><a href="participants.html">For participants</a></li>
<li><a href="uses_of_llms_in_hackathons.html">Uses of LLMs in hackathons</a></li>
<li><a href="prototyping.html">LLMs for prototyping</a></li>
<li><a href="ethical_considerations_overview.html">Ethics overview</a></li>
<li><a href="questions_limitations_use_llms.html">Ethics_questions_of_LLM_use</a></li>
<li><a href="guidance_ethics_of_llm_use.html">Ethics guidance</a></li>
    </ul>
</nav>

      <div id="llm4hack_content">
          <h1><strong>Guidelines for Ethical Use of LLMs in Hackathons</strong></h1>
<h2><strong>Topic Areas</strong></h2>
<ul>
<li>Respect privacy, intellectual property, and licensing terms</li>
<li>Reduce bias and increase inclusivity</li>
<li>Environmental Impacts and Long-term Usability Risks</li>
<li>Identify LLM use</li>
<li>Disclose risks and reliability of LLM output</li>
</ul>
<h3><strong>1. Respect Privacy, Intellectual Property, and Licensing Terms</strong></h3>
<h4><strong>Key Concerns</strong></h4>
<ol>
<li>Using restricted or copyrighted data without permission or violating licensing terms.</li>
<li>Mishandling sensitive user data, such as collecting or storing it without proper safeguards.</li>
<li>Misattributing or misusing intellectual property in solutions developed during the hackathon.</li>
<li>Not stating origins of datasets used, i.e. public, private, government, science</li>
</ol>
<h4><strong>Guidelines</strong></h4>
<ol>
<li>Verify licensing and permissions for datasets</li>
<li>Minimize data collection and ensure encryption for sensitive data.</li>
<li>Use synthetic data when necessary when can't ensure data will be private</li>
<li>Attribute all borrowed content and clarify IP ownership with sponsors or organizers.</li>
<li>State source of dataset</li>
</ol>
<h4><strong>Origins of concern</strong></h4>
<h5><strong>Privacy Concerns</strong></h5>
<ul>
<li><strong>Inadvertent Data Leakage</strong>: LLMs trained on unfiltered datasets might inadvertently generate sensitive or personal information if it was present in the training data (e.g., PII like names, addresses, or emails).</li>
<li><strong>Data Collection Missteps</strong>: Applications that collect user inputs for generating LLM responses might inadvertently store sensitive information without proper consent or safeguards.</li>
<li><strong>Re-identification Risks</strong>: Even anonymized outputs could be combined with other data sources to re-identify individuals, posing a privacy violation.</li>
</ul>
<h5><strong>Intellectual Property (IP) Concerns</strong></h5>
<ul>
<li><strong>Generation of Copyrighted Content</strong>: LLMs may produce text, code, or creative works that closely resemble copyrighted material found in the training data, potentially leading to copyright infringement.</li>
<li><strong>Unauthorized Use of Proprietary Knowledge</strong>: If an LLM was trained on proprietary or confidential datasets without permission, its outputs could reveal protected trade secrets or proprietary information.</li>
<li><strong>Ambiguity in Ownership</strong>: Outputs generated by LLMs can create legal uncertainty about IP ownership, especially in collaborative or commercial contexts.</li>
</ul>
<h5><strong>Licensing Concerns</strong></h5>
<ul>
<li><strong>Improper Dataset Usage</strong>: If training data or third-party models used to fine-tune the LLM were not properly licensed, the downstream outputs might violate the licensing terms.</li>
<li><strong>Unacknowledged Dependencies</strong>: Applications may inadvertently omit attribution or fail to comply with open-source or Creative Commons licensing requirements for datasets or models used.</li>
<li><strong>Commercial Use Restrictions</strong>: LLM-generated outputs might rely on non-commercially licensed data, leading to legal challenges if used in public or for-profit applications.</li>
</ul>
<h4><strong>Mitigation Strategies</strong></h4>
<h5><strong>Privacy Mitigation</strong></h5>
<ol>
<li><strong>Filter Training Data</strong>: Remove sensitive or identifiable information from training datasets to minimize the risk of leakage in generated outputs.</li>
<li><strong>Implement User Safeguards</strong>:</li>
<li>Anonymize and securely store any user inputs collected for application use.</li>
<li>Clearly communicate data usage policies and obtain explicit consent from users.</li>
<li><strong>Regular Audits</strong>: Conduct regular privacy audits of the application and model outputs to identify and mitigate privacy risks, including re-identification.</li>
</ol>
<h5><strong>Intellectual Property Mitigation</strong></h5>
<ol>
<li><strong>Verify Training Data</strong>:</li>
<li>Use datasets that are publicly available, open-source, or appropriately licensed for use in training.</li>
<li>Avoid proprietary data unless explicit permissions are granted.</li>
<li><strong>Post-Processing Filters</strong>:</li>
<li>Use automated filters or human reviewers to detect and exclude outputs that closely resemble copyrighted material.</li>
<li>Avoid deploying outputs in sensitive applications without review.</li>
<li><strong>Clarify IP Ownership</strong>: Establish clear policies for the ownership of LLM-generated outputs, ensuring all parties involved (e.g., users, developers) understand their rights and responsibilities.</li>
</ol>
<h5><strong>Licensing Mitigation</strong></h5>
<ol>
<li><strong>Compliance with Licensing Terms</strong>:</li>
<li>Review and adhere to all licensing requirements for training datasets, fine-tuning models, or LLM components.</li>
<li>Attribute any open-source or Creative Commons content as required.</li>
<li><strong>Limit Use of Questionable Outputs</strong>: Avoid relying on outputs that could conflict with the licensing terms of the model or its data sources, especially in commercial applications.</li>
<li><strong>Legal Review</strong>:</li>
<li>Consult legal experts to ensure compliance with data use and licensing agreements.</li>
<li>Include clear disclaimers about the origins and limitations of LLM outputs in the application documentation.</li>
</ol>
<h3><strong>2. Reduce bias and increase inclusivity</strong></h3>
<h4><strong>Key Concerns</strong></h4>
<ol>
<li>Reinforcing biases in conclusions</li>
<li>Excluding certain groups in solutions</li>
</ol>
<h4><strong>Guidelines</strong></h4>
<ol>
<li>Test solutions for biases and ensure compliance with accessibility standards (e.g., WCAG).</li>
<li>State that outputs are experimental and may include biases or errors.</li>
</ol>
<h4><strong>Origins of concern</strong></h4>
<ol>
<li><strong>Bias in Training Data</strong></li>
<li><strong>Overrepresentation or Underrepresentation</strong>: If the training data overrepresents certain groups while underrepresenting others, the model's outputs may disproportionately favor the dominant groups.</li>
<li><strong>Historical Bias</strong>: Data reflecting historical inequities (e.g., gender roles, racial discrimination) can lead to the perpetuation of those biases in generated outputs.</li>
<li>
<p><strong>Sampling Bias</strong>: Data collected from specific regions, platforms, or demographics may exclude perspectives from underrepresented or marginalized communities.</p>
</li>
<li>
<p><strong>Model Behavior and Outputs</strong></p>
</li>
<li><strong>Amplification of Biases</strong>: LLMs can amplify subtle biases present in the training data. For example, if the data contains biased language or stereotypes, the LLM may generate content that reinforces those ideas.</li>
<li>
<p><strong>Stereotype Generation</strong>: LLMs often rely on probabilities from training data, which may lead them to default to stereotypical or exclusionary outputs when ambiguous inputs are provided.</p>
</li>
<li>
<p><strong>Limited Context Awareness</strong></p>
</li>
<li><strong>Failure to Adapt to Nuances</strong>: LLMs may not fully understand cultural or contextual nuances, leading to outputs that unintentionally alienate or misrepresent certain groups.</li>
<li>
<p><strong>Lack of Real-World Understanding</strong>: LLMs lack empathy or intent, so they cannot recognize or avoid perpetuating harmful patterns unless explicitly corrected through guidelines or data augmentation.</p>
</li>
<li>
<p><strong>Application Design Choices</strong></p>
</li>
<li><strong>Insufficient Testing for Biases</strong>: Applications using LLMs may not adequately test for biases across diverse demographics, leading to unintended exclusions or harmful outputs.</li>
<li><strong>One-size-fits-all Approaches</strong>: Solutions may fail to account for cultural, linguistic, or accessibility differences, leading to suboptimal results for minority or underrepresented groups.</li>
</ol>
<h4><strong>Mitigation Strategies</strong></h4>
<ol>
<li><strong>Curating Training Data</strong></li>
<li>Use balanced datasets that represent diverse groups equitably.</li>
<li>
<p>Actively identify and remove harmful or biased content during preprocessing.</p>
</li>
<li>
<p><strong>Fine-tuning for Fairness</strong></p>
</li>
<li>Fine-tune LLMs with additional datasets that emphasize inclusivity and mitigate biases.</li>
<li>
<p>Incorporate fairness-aware algorithms during model training and inference.</p>
</li>
<li>
<p><strong>Bias Audits and Testing</strong></p>
</li>
<li>Conduct regular audits of the model's outputs to identify biased or exclusionary behaviors.</li>
<li>
<p>Use synthetic datasets or counterfactual testing to evaluate performance across demographics.</p>
</li>
<li>
<p><strong>Incorporating Feedback Loops</strong></p>
</li>
<li>Allow users to report biased or harmful outputs, and use this feedback to improve future iterations.</li>
<li>
<p>Include human review mechanisms for sensitive or high-stakes applications.</p>
</li>
<li>
<p><strong>Designing for Inclusivity</strong></p>
</li>
<li>Consider language, accessibility, and cultural sensitivities during application design.</li>
<li>Use universal design principles to ensure solutions are usable by as many people as possible.</li>
</ol>
<p>By acknowledging and addressing these factors, developers can reduce the risk of reinforcing biases or excluding certain groups when deploying LLM-based solutions.</p>
<h3><strong>3. Environmental Impacts and Long-term Usability Risks</strong></h3>
<h4><strong>Key Concerns</strong></h4>
<ol>
<li>Ignoring environmental impacts.</li>
<li>Overlooking the long-term usability or maintenance of projects.</li>
</ol>
<h4><strong>Guidelines</strong></h4>
<ol>
<li>Optimize computational efficiency to reduce environmental impact.</li>
<li>State any issues that could lead to the application breaking in the future.</li>
</ol>
<h4><strong>Origins of concern</strong></h4>
<h5><strong>Environmental Impacts</strong></h5>
<ul>
<li><strong>High Energy Consumption</strong>: The computational resources required to run LLMs, especially for real-time public applications, are energy-intensive and may contribute significantly to carbon emissions.</li>
<li><strong>Inefficient Usage</strong>: Applications might use LLMs for tasks where simpler, less resource-intensive solutions could suffice, wasting resources unnecessarily.</li>
<li><strong>Scaling Challenges</strong>: Increased usage in public applications can exponentially raise resource demands, compounding environmental impacts.</li>
</ul>
<h5><strong>Long-term Usability and Maintenance</strong></h5>
<ul>
<li><strong>Dependence on Proprietary Models</strong>: Relying on external APIs or proprietary models without guaranteed availability or support can lead to obsolescence if the provider changes terms, increases costs, or discontinues services.</li>
<li><strong>Lack of Scalability</strong>: Applications might fail to adapt to growing user bases due to insufficient planning for scalability and maintainability.</li>
<li><strong>Technical Debt</strong>: Overlooking proper documentation, modular design, or regular updates can result in systems that are hard to maintain, extend, or debug over time.</li>
</ul>
<h4><strong>Mitigation Strategies</strong></h4>
<h5><strong>Addressing Environmental Impacts</strong></h5>
<ol>
<li><strong>Optimize Resource Use</strong>:</li>
<li>Use smaller, fine-tuned models for tasks that do not require full-scale LLM capabilities.</li>
<li>Implement caching or pre-computation strategies to reduce redundant queries to LLMs.</li>
<li><strong>Choose Sustainable Infrastructure</strong>:</li>
<li>Deploy models on energy-efficient cloud providers or infrastructure powered by renewable energy.</li>
<li>Monitor and optimize resource usage to minimize energy consumption.</li>
<li><strong>Encourage Conscious Usage</strong>:</li>
<li>Design the application to discourage unnecessary or excessive use of LLM-powered features.</li>
<li>Educate users about the environmental costs associated with resource-intensive tasks.</li>
</ol>
<h5><strong>Ensuring Long-term Usability and Maintenance</strong></h5>
<ol>
<li><strong>Plan for Longevity</strong>:</li>
<li>Build modular systems that allow components to be updated or replaced independently of the core application.</li>
<li>Use open standards and widely supported technologies to reduce reliance on proprietary solutions.</li>
<li><strong>Document and Test</strong>:</li>
<li>Maintain thorough documentation for the application’s architecture, codebase, and dependencies.</li>
<li>Conduct regular testing and updates to ensure compatibility with new technologies and standards.</li>
<li><strong>Design for Scalability</strong>:</li>
<li>Plan for increased usage by implementing scalable infrastructure and load-balancing mechanisms.</li>
<li>Monitor performance and capacity, adjusting resources proactively to meet user demand.</li>
</ol>
<h3><strong>4. Identify LLM use</strong></h3>
<h4><strong>Key Concerns</strong></h4>
<ol>
<li>Misleading stakeholders by not stating how the LLM was used in creating the project.</li>
<li>Misrepresenting LLM use as original work.</li>
</ol>
<h4><strong>Guidelines</strong></h4>
<ol>
<li>Document tools, data sources, and the role of LLMs in the project.</li>
<li>Distinguish between LLM input and user input regarding functional features and conceptual ideas.</li>
<li>Disclose LLM use and ensure most work is original.</li>
</ol>
<h4><strong>Origins of concern</strong></h4>
<h5><strong>Overstating Performance and Functionality</strong></h5>
<ul>
<li><strong>Prototype Misrepresentation</strong>: Projects may present outputs as if they are fully functional when they are dependent on pre-generated or limited LLM capabilities, creating a false perception of the project’s maturity.</li>
<li><strong>Ignoring Limitations</strong>: Participants may fail to disclose known limitations of the LLM, such as inaccuracies, biases, or dependency on specific data inputs.</li>
<li><strong>Falsified Results</strong>: By leveraging LLMs to generate polished outputs (e.g., code, datasets, or design concepts), teams might falsely claim higher levels of development or complexity than were achieved during the hackathon.</li>
</ul>
<h5><strong>Misrepresenting LLM Contributions</strong></h5>
<ul>
<li><strong>Obscuring Automation</strong>: Teams might downplay or hide the extent to which the LLM generated key parts of the project, misrepresenting the originality or effort involved.</li>
<li><strong>Attributing Work to Participants</strong>: Outputs created primarily by an LLM (e.g., written content, code, or design elements) might be falsely attributed to team members, giving a misleading impression of their skill or contribution.</li>
<li><strong>Improper Use of Pre-Trained Models</strong>: Leveraging pre-trained LLMs without appropriate disclosure could lead judges or stakeholders to assume the work was created from scratch during the hackathon.</li>
</ul>
<h4><strong>Mitigation Strategies</strong></h4>
<h5><strong>Addressing Overstated Performance and Functionality</strong></h5>
<ol>
<li><strong>Transparent Presentations</strong>:</li>
<li>Clearly differentiate between implemented features and conceptual ideas in pitches and demonstrations.</li>
<li>Label any outputs or functionality generated by LLMs as experimental or prototype-level, emphasizing their current limitations.</li>
<li><strong>Document Limitations</strong>:</li>
<li>Include explicit disclaimers about the LLM’s weaknesses (e.g., bias, accuracy issues, or context dependencies) in project documentation.</li>
<li>Share insights into potential edge cases or failure modes to give a realistic picture of performance.</li>
<li><strong>Focus on Next Steps</strong>:</li>
<li>Acknowledge areas where further development is needed to transition from a prototype to a viable solution.</li>
</ol>
<h5><strong>Ensuring Honest Attribution of LLM Contributions</strong></h5>
<ol>
<li><strong>Disclose LLM Usage</strong>:</li>
<li>Clearly state how the LLM was used in the project (e.g., generating text, coding, creating datasets) during the pitch and in project documentation.</li>
<li>Provide examples of specific outputs created by the LLM versus those created manually by the team.</li>
<li><strong>Highlight Team Efforts</strong>:</li>
<li>Focus on the team’s unique contributions, such as how they integrated the LLM into the solution, customized its outputs, or designed systems around it.</li>
<li>Avoid claiming credit for tasks wholly performed by the LLM, such as generating large sections of code or writing entire documents.</li>
<li><strong>Set Guidelines for Ethical Use</strong>:</li>
<li>Adhere to hackathon rules about originality and external tool usage, ensuring that LLM-generated work aligns with event guidelines.</li>
<li>Transparently disclose all external tools, models, and datasets used in the project.</li>
</ol>
<h3><strong>5. Disclose risks and reliability of LLM output</strong></h3>
<h4><strong>Key Concerns</strong></h4>
<ol>
<li>Withholding risks, limitations, or known issues during presentations.</li>
<li>Misleading users about the accuracy and reliability of LLM-powered features.</li>
<li>Developing applications that lack fairness.</li>
<li>Ignoring the risks of LLM "hallucinations" (producing false or misleading information).</li>
</ol>
<h4><strong>Guidelines</strong></h4>
<ol>
<li>Proactively disclose risks, including potential failure scenarios.</li>
<li>Avoid creating tools with malicious or unethical applications.</li>
<li>Consider the societal and lifecycle impacts of the solution.</li>
</ol>
<h4><strong>Origins of concern</strong></h4>
<h5><strong>Withholding Risks</strong></h5>
<ul>
<li><strong>Unstated Biases</strong>: LLM outputs often inherit biases from training data. Teams may fail to disclose these biases, leading users to believe the outputs are impartial or universally applicable.</li>
<li><strong>Unexplained Edge Cases</strong>: LLMs can produce unreliable or nonsensical outputs in specific scenarios, which might be overlooked or hidden in project demonstrations.</li>
<li><strong>Omitted Ethical Concerns</strong>: Participants might not discuss potential ethical risks, such as misuse of the LLM for malicious purposes or the generation of harmful or misleading content.</li>
</ul>
<h5><strong>Misleading Users About Reliability</strong></h5>
<ul>
<li><strong>Overconfidence in Outputs</strong>: Presenting LLM responses as definitive or authoritative, without clarifying their probabilistic nature, may lead users to overly trust the outputs.</li>
<li><strong>Hiding Model Limitations</strong>: Teams may avoid discussing issues like hallucinations (confidently generating false information), reliance on incomplete datasets, or lack of domain-specific expertise.</li>
<li><strong>Failure to Acknowledge Developmental Stage</strong>: Projects may present prototype-level solutions as production-ready, glossing over the immaturity or incompleteness of the system.</li>
</ul>
<h4><strong>Mitigation Strategies</strong></h4>
<h5><strong>Addressing Withheld Risks</strong></h5>
<ol>
<li><strong>Explicitly Acknowledge Biases</strong>:</li>
<li>Document and discuss potential biases inherent in the LLM’s training data, especially those relevant to the project domain.</li>
<li>Share examples of biased or problematic outputs observed during development to raise awareness.</li>
<li><strong>Include Ethical Risk Disclosures</strong>:</li>
<li>Outline potential misuse scenarios and any safeguards implemented to mitigate such risks.</li>
<li>Highlight unresolved ethical concerns and suggest areas for future work to address them.</li>
<li><strong>Test and Report Edge Cases</strong>:</li>
<li>Actively test the LLM in diverse scenarios to identify edge cases where outputs may be unreliable.</li>
<li>Share findings transparently, emphasizing known failure points or limitations.</li>
</ol>
<h5><strong>Improving Transparency About Reliability</strong></h5>
<ol>
<li><strong>Use Disclaimers</strong>:</li>
<li>Add clear disclaimers in presentations, documentation, and user interfaces, indicating that LLM outputs may be incomplete, biased, or inaccurate.</li>
<li>Example disclaimer: <em>“This tool is a prototype relying on probabilistic models and may produce inaccurate or unexpected outputs.”</em></li>
<li><strong>Focus on Probabilistic Nature</strong>:</li>
<li>Emphasize that LLMs generate outputs based on likelihoods rather than deterministic logic, and results should be verified in high-stakes applications.</li>
<li><strong>Clarify Prototype Status</strong>:</li>
<li>Transparently communicate the current development stage of the solution, including areas requiring further testing or refinement.</li>
<li>Frame limitations as opportunities for future work, demonstrating awareness and a plan for addressing them.</li>
</ol>
      </div>
    </div>
</body>
</html>
