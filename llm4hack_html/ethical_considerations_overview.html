
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLMs in Hackathons</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&family=Raleway:ital@0;1&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="llm4hack_styles.css" >
</head>
<body>
    
<nav id="llm4hack_nav">
    <ul>
        <li><a href="home.html">Home</a></li>
<li><a href="llms_for_hackathons.html">LLMs for hackathons</a></li>
<li><a href="organizers.html">For organizers</a></li>
<li><a href="participants.html">For participants</a></li>
<li><a href="uses_of_llms_in_hackathons.html">Uses of LLMs in hackathons</a></li>
<li><a href="prototyping.html">LLMs for prototyping</a></li>
<li><a href="ethical_considerations_overview.html">Ethics overview</a></li>
<li><a href="questions_limitations_use_llms.html">Ethics_questions_of_LLM_use</a></li>
<li><a href="guidance_ethics_of_llm_use.html">Ethics guidance</a></li>
    </ul>
</nav>

    <div id="llm4hack_content">
        <h1>Overview of Ethical Considerations</h1>
<h1>Ethical Considerations for Organizers and Participants</h1>
<h3>Participants</h3>
<ul>
<li><strong>Idea Generation</strong>:</li>
<li>Avoid plagiarism and ensure attribution.</li>
<li><strong>Coding Assistance</strong>:</li>
<li>Verify generated code for originality and bias.</li>
<li><strong>Content Creation</strong>:</li>
<li>Ensure factual accuracy and originality.</li>
<li><strong>Collaboration</strong>:</li>
<li>Communicate transparently about LLM usage.</li>
</ul>
<h3>Organizers</h3>
<ul>
<li><strong>Event Management</strong>:</li>
<li>Ensure fairness and transparency in automation.</li>
<li><strong>Participant Support</strong>:</li>
<li>Train LLMs to provide respectful and accurate support.</li>
<li><strong>Judging</strong>:</li>
<li>Use LLM evaluations as supplementary tools, not replacements.</li>
</ul>
<h2>Additional Ethical considerations</h2>
<p>If competition is involved, does using an LLM constitute 'cheating'?
it is important for organizers to be clear on rules and expectations
* How can participation rules address the use of LLMs? (to even the playing fields for all participants?)
Add - Intellectual property
What are additional considerations and concerns regarding authorship of code/output that come from LLMs
sharing data with LLMs that is sensitive or you don't own - this becomes part of the training data (!!!!)</p>
<h2>5 ethical considerations from papers</h2>
<h3>Based on the papers below here are 5 ethical considerations:</h3>
<ol>
<li>
<p><strong>Bias in Outputs</strong>
   LLMs may produce outputs that reflect inherent biases in their training data, which can unintentionally perpetuate stereotypes or inequalities in hackathon solutions
   <a href="https://ar5iv.org/abs/2406.11400">ar5iv</a> <a href="https://ar5iv.org/abs/2306.06283">ar5iv</a></p>
</li>
<li>
<p><strong>Transparency in Usage</strong>
   Hackathon teams should clearly communicate how LLMs are used, ensuring participants and stakeholders understand AI's role in generating ideas or solutions
   <a href="https://ar5iv.org/abs/2405.14445">ar5iv</a> <a href="https://ar5iv.org/abs/2306.06283">ar5iv</a></p>
</li>
<li>2a. Is there an element of "shaming" with admitting to LLM use? Is there a way to survey anonymously to avoid the double bind?</li>
<li>2b. Organizers should provide guidance on LLM use, including ethical considerations. This serves as an opportunity for education around ethical use of LLMs.</li>
<li>
<p>2c. The user is still responsible for outputs.</p>
</li>
<li>
<p><strong>Over-reliance on AI</strong>
   Excessive dependence on LLMs can lead to reduced critical thinking or failure to scrutinize AI-generated outputs, especially when data accuracy is crucial
   <a href="https://ar5iv.org/pdf/2405.14445">ar5iv</a> <a href="https://ar5iv.org/pdf/2306.06283v3">ar5iv</a></p>
</li>
<li>
<p><strong>Safety and Misinformation</strong>
   LLMs can produce incorrect or misleading information if not properly guided, posing risks in sensitive fields like healthcare, education, or scientific research
   <a href="https://ar5iv.org/abs/2406.11400">ar5iv</a> <a href="https://ar5iv.org/pdf/2306.06283v3">ar5iv</a></p>
</li>
<li>
<p><strong>Ethical Accessibility</strong>
   LLM-based tools should be designed to ensure inclusivity and fairness, avoiding barriers for less technically proficient participants or marginalized groups
   <a href="https://ar5iv.org/abs/2406.11400">ar5iv</a> <a href="https://ar5iv.org/pdf/2306.06283v3">ar5iv</a></p>
</li>
<li>5a. Groups with paid subscriptions might have access to better models and have an advantage.</li>
</ol>
<h3>5 Papers on arXiv That Have “LLMs” and “Hackathon” in the Abstract:</h3>
<ul>
<li>
<p><strong>arXiv:2408.08878</strong> [pdf, other]
  <em>Knowledge Prompting: How Knowledge Engineers Use Large Language Models</em>
  Authors: Elisavet Koutsiana et al.
  Submitted: 2 August, 2024; originally announced: August 2024.</p>
</li>
<li>
<p><strong>arXiv:2406.11400</strong> [pdf, other]
  <em>Large Language Models and Knowledge Graphs for Astronomical Entity Disambiguation</em>
  Authors: Golnaz Shapurian
  Submitted: 17 June, 2024; originally announced: June 2024.</p>
</li>
<li>
<p><strong>arXiv:2405.14445</strong> [pdf]
  <em>Exploring the Use of a Large Language Model for Data Extraction in Systematic Reviews: A Rapid Feasibility Study</em>
  Authors: Lena Schmidt et al.
  Abstract: This paper describes a rapid feasibility study of using GPT-4, a large...
  Journal Ref: Proceedings of the 3rd Workshop on Augmented Intelligence for Technology-Assisted Reviews Systems, 2024.</p>
</li>
<li>
<p><strong>arXiv:2404.18479</strong> [pdf]
  <em>ChatGPT as an Inventor: Eliciting the Strengths and Weaknesses of Current Large Language Models Against Humans in Engineering Design</em>
  Authors: Daniel Nygård Ege et al.
  Submitted: 29 April, 2024; originally announced: April 2024.</p>
</li>
<li>
<p><strong>arXiv:2306.06283</strong> [pdf, other]
  <em>14 Examples of How LLMs Can Transform Materials Science and Chemistry: A Reflection on a Large Language Model Hackathon</em>
  Authors: Kevin Maik Jablonka et al.
  Submitted: 14 July, 2023; v1 submitted: 9 June, 2023; originally announced: June 2023.</p>
</li>
</ul>
    </div>
</body>
</html>
