# **Guidelines for Ethical Use of LLMs in Hackathons**

## **Topic Areas**
- Respect privacy, intellectual property, and licensing terms
- Reduce bias and increase inclusivity
- Environmental Impacts and Long-term Usability Risks
- Identify LLM use
- Disclose risks and reliability of LLM output

### **1. Respect Privacy, Intellectual Property, and Licensing Terms**

#### **Key Concerns**
1. Using restricted or copyrighted data without permission or violating licensing terms.
2. Mishandling sensitive user data, such as collecting or storing it without proper safeguards.
3. Misattributing or misusing intellectual property in solutions developed during the hackathon.
4. Not stating origins of datasets used, i.e. public, private, government, science

#### **Guidelines**
1. Verify licensing and permissions for datasets
2. Minimize data collection and ensure encryption for sensitive data.
3. Use synthetic data when necessary when can't ensure data will be private
3. Attribute all borrowed content and clarify IP ownership with sponsors or organizers.
4. State source of dataset

#### **Origins of concern**

##### **Privacy Concerns**
- **Inadvertent Data Leakage**: LLMs trained on unfiltered datasets might inadvertently generate sensitive or personal information if it was present in the training data (e.g., PII like names, addresses, or emails).
- **Data Collection Missteps**: Applications that collect user inputs for generating LLM responses might inadvertently store sensitive information without proper consent or safeguards.
- **Re-identification Risks**: Even anonymized outputs could be combined with other data sources to re-identify individuals, posing a privacy violation.

##### **Intellectual Property (IP) Concerns**
- **Generation of Copyrighted Content**: LLMs may produce text, code, or creative works that closely resemble copyrighted material found in the training data, potentially leading to copyright infringement.
- **Unauthorized Use of Proprietary Knowledge**: If an LLM was trained on proprietary or confidential datasets without permission, its outputs could reveal protected trade secrets or proprietary information.
- **Ambiguity in Ownership**: Outputs generated by LLMs can create legal uncertainty about IP ownership, especially in collaborative or commercial contexts.

##### **Licensing Concerns**
- **Improper Dataset Usage**: If training data or third-party models used to fine-tune the LLM were not properly licensed, the downstream outputs might violate the licensing terms.
- **Unacknowledged Dependencies**: Applications may inadvertently omit attribution or fail to comply with open-source or Creative Commons licensing requirements for datasets or models used.
- **Commercial Use Restrictions**: LLM-generated outputs might rely on non-commercially licensed data, leading to legal challenges if used in public or for-profit applications.

#### **Mitigation Strategies**

##### **Privacy Mitigation**
1. **Filter Training Data**: Remove sensitive or identifiable information from training datasets to minimize the risk of leakage in generated outputs.
2. **Implement User Safeguards**:
   - Anonymize and securely store any user inputs collected for application use.
   - Clearly communicate data usage policies and obtain explicit consent from users.
3. **Regular Audits**: Conduct regular privacy audits of the application and model outputs to identify and mitigate privacy risks, including re-identification.

##### **Intellectual Property Mitigation**
1. **Verify Training Data**:
   - Use datasets that are publicly available, open-source, or appropriately licensed for use in training.
   - Avoid proprietary data unless explicit permissions are granted.
2. **Post-Processing Filters**:
   - Use automated filters or human reviewers to detect and exclude outputs that closely resemble copyrighted material.
   - Avoid deploying outputs in sensitive applications without review.
3. **Clarify IP Ownership**: Establish clear policies for the ownership of LLM-generated outputs, ensuring all parties involved (e.g., users, developers) understand their rights and responsibilities.

##### **Licensing Mitigation**
1. **Compliance with Licensing Terms**:
   - Review and adhere to all licensing requirements for training datasets, fine-tuning models, or LLM components.
   - Attribute any open-source or Creative Commons content as required.
2. **Limit Use of Questionable Outputs**: Avoid relying on outputs that could conflict with the licensing terms of the model or its data sources, especially in commercial applications.
3. **Legal Review**:
   - Consult legal experts to ensure compliance with data use and licensing agreements.
   - Include clear disclaimers about the origins and limitations of LLM outputs in the application documentation.

### **2. Reduce bias and increase inclusivity**

#### **Key Concerns**
1. Reinforcing biases in conclusions
2. Excluding certain groups in solutions

#### **Guidelines**
1. Test solutions for biases and ensure compliance with accessibility standards (e.g., WCAG).
2. State that outputs are experimental and may include biases or errors.

#### **Origins of concern**
1. **Bias in Training Data**
   - **Overrepresentation or Underrepresentation**: If the training data overrepresents certain groups while underrepresenting others, the model's outputs may disproportionately favor the dominant groups.
   - **Historical Bias**: Data reflecting historical inequities (e.g., gender roles, racial discrimination) can lead to the perpetuation of those biases in generated outputs.
   - **Sampling Bias**: Data collected from specific regions, platforms, or demographics may exclude perspectives from underrepresented or marginalized communities.

2. **Model Behavior and Outputs**
   - **Amplification of Biases**: LLMs can amplify subtle biases present in the training data. For example, if the data contains biased language or stereotypes, the LLM may generate content that reinforces those ideas.
   - **Stereotype Generation**: LLMs often rely on probabilities from training data, which may lead them to default to stereotypical or exclusionary outputs when ambiguous inputs are provided.

3. **Limited Context Awareness**
   - **Failure to Adapt to Nuances**: LLMs may not fully understand cultural or contextual nuances, leading to outputs that unintentionally alienate or misrepresent certain groups.
   - **Lack of Real-World Understanding**: LLMs lack empathy or intent, so they cannot recognize or avoid perpetuating harmful patterns unless explicitly corrected through guidelines or data augmentation.

4. **Application Design Choices**
   - **Insufficient Testing for Biases**: Applications using LLMs may not adequately test for biases across diverse demographics, leading to unintended exclusions or harmful outputs.
   - **One-size-fits-all Approaches**: Solutions may fail to account for cultural, linguistic, or accessibility differences, leading to suboptimal results for minority or underrepresented groups.

#### **Mitigation Strategies**
1. **Curating Training Data**
   - Use balanced datasets that represent diverse groups equitably.
   - Actively identify and remove harmful or biased content during preprocessing.

2. **Fine-tuning for Fairness**
   - Fine-tune LLMs with additional datasets that emphasize inclusivity and mitigate biases.
   - Incorporate fairness-aware algorithms during model training and inference.

3. **Bias Audits and Testing**
   - Conduct regular audits of the model's outputs to identify biased or exclusionary behaviors.
   - Use synthetic datasets or counterfactual testing to evaluate performance across demographics.

4. **Incorporating Feedback Loops**
   - Allow users to report biased or harmful outputs, and use this feedback to improve future iterations.
   - Include human review mechanisms for sensitive or high-stakes applications.

5. **Designing for Inclusivity**
   - Consider language, accessibility, and cultural sensitivities during application design.
   - Use universal design principles to ensure solutions are usable by as many people as possible.

By acknowledging and addressing these factors, developers can reduce the risk of reinforcing biases or excluding certain groups when deploying LLM-based solutions.


### **3. Environmental Impacts and Long-term Usability Risks**

#### **Key Concerns**
1. Ignoring environmental impacts.
2. Overlooking the long-term usability or maintenance of projects.

#### **Guidelines**
1. Optimize computational efficiency to reduce environmental impact.
2. State any issues that could lead to the application breaking in the future.

#### **Origins of concern**

##### **Environmental Impacts**
- **High Energy Consumption**: The computational resources required to run LLMs, especially for real-time public applications, are energy-intensive and may contribute significantly to carbon emissions.
- **Inefficient Usage**: Applications might use LLMs for tasks where simpler, less resource-intensive solutions could suffice, wasting resources unnecessarily.
- **Scaling Challenges**: Increased usage in public applications can exponentially raise resource demands, compounding environmental impacts.

##### **Long-term Usability and Maintenance**
- **Dependence on Proprietary Models**: Relying on external APIs or proprietary models without guaranteed availability or support can lead to obsolescence if the provider changes terms, increases costs, or discontinues services.
- **Lack of Scalability**: Applications might fail to adapt to growing user bases due to insufficient planning for scalability and maintainability.
- **Technical Debt**: Overlooking proper documentation, modular design, or regular updates can result in systems that are hard to maintain, extend, or debug over time.

#### **Mitigation Strategies**

##### **Addressing Environmental Impacts**
1. **Optimize Resource Use**:
   - Use smaller, fine-tuned models for tasks that do not require full-scale LLM capabilities.
   - Implement caching or pre-computation strategies to reduce redundant queries to LLMs.
2. **Choose Sustainable Infrastructure**:
   - Deploy models on energy-efficient cloud providers or infrastructure powered by renewable energy.
   - Monitor and optimize resource usage to minimize energy consumption.
3. **Encourage Conscious Usage**:
   - Design the application to discourage unnecessary or excessive use of LLM-powered features.
   - Educate users about the environmental costs associated with resource-intensive tasks.

##### **Ensuring Long-term Usability and Maintenance**
1. **Plan for Longevity**:
   - Build modular systems that allow components to be updated or replaced independently of the core application.
   - Use open standards and widely supported technologies to reduce reliance on proprietary solutions.
2. **Document and Test**:
   - Maintain thorough documentation for the application’s architecture, codebase, and dependencies.
   - Conduct regular testing and updates to ensure compatibility with new technologies and standards.
3. **Design for Scalability**:
   - Plan for increased usage by implementing scalable infrastructure and load-balancing mechanisms.
   - Monitor performance and capacity, adjusting resources proactively to meet user demand.


### **4. Identify LLM use**

#### **Key Concerns**
1. Misleading stakeholders by not stating how the LLM was used in creating the project.
2. Misrepresenting LLM use as original work.

#### **Guidelines**
1. Document tools, data sources, and the role of LLMs in the project.
2. Distinguish between LLM input and user input regarding functional features and conceptual ideas.
3. Disclose LLM use and ensure most work is original.

#### **Origins of concern**

##### **Overstating Performance and Functionality**
- **Prototype Misrepresentation**: Projects may present outputs as if they are fully functional when they are dependent on pre-generated or limited LLM capabilities, creating a false perception of the project’s maturity.
- **Ignoring Limitations**: Participants may fail to disclose known limitations of the LLM, such as inaccuracies, biases, or dependency on specific data inputs.
- **Falsified Results**: By leveraging LLMs to generate polished outputs (e.g., code, datasets, or design concepts), teams might falsely claim higher levels of development or complexity than were achieved during the hackathon.

##### **Misrepresenting LLM Contributions**
- **Obscuring Automation**: Teams might downplay or hide the extent to which the LLM generated key parts of the project, misrepresenting the originality or effort involved.
- **Attributing Work to Participants**: Outputs created primarily by an LLM (e.g., written content, code, or design elements) might be falsely attributed to team members, giving a misleading impression of their skill or contribution.
- **Improper Use of Pre-Trained Models**: Leveraging pre-trained LLMs without appropriate disclosure could lead judges or stakeholders to assume the work was created from scratch during the hackathon.

#### **Mitigation Strategies**

##### **Addressing Overstated Performance and Functionality**
1. **Transparent Presentations**:
   - Clearly differentiate between implemented features and conceptual ideas in pitches and demonstrations.
   - Label any outputs or functionality generated by LLMs as experimental or prototype-level, emphasizing their current limitations.
2. **Document Limitations**:
   - Include explicit disclaimers about the LLM’s weaknesses (e.g., bias, accuracy issues, or context dependencies) in project documentation.
   - Share insights into potential edge cases or failure modes to give a realistic picture of performance.
3. **Focus on Next Steps**:
   - Acknowledge areas where further development is needed to transition from a prototype to a viable solution.

##### **Ensuring Honest Attribution of LLM Contributions**
1. **Disclose LLM Usage**:
   - Clearly state how the LLM was used in the project (e.g., generating text, coding, creating datasets) during the pitch and in project documentation.
   - Provide examples of specific outputs created by the LLM versus those created manually by the team.
2. **Highlight Team Efforts**:
   - Focus on the team’s unique contributions, such as how they integrated the LLM into the solution, customized its outputs, or designed systems around it.
   - Avoid claiming credit for tasks wholly performed by the LLM, such as generating large sections of code or writing entire documents.
3. **Set Guidelines for Ethical Use**:
   - Adhere to hackathon rules about originality and external tool usage, ensuring that LLM-generated work aligns with event guidelines.
   - Transparently disclose all external tools, models, and datasets used in the project.


### **5. Disclose risks and reliability of LLM output**

#### **Key Concerns**
1. Withholding risks, limitations, or known issues during presentations.
2. Misleading users about the accuracy and reliability of LLM-powered features.
3. Developing applications that lack fairness.
4. Ignoring the risks of LLM "hallucinations" (producing false or misleading information).

#### **Guidelines**
2. Proactively disclose risks, including potential failure scenarios.
3. Avoid creating tools with malicious or unethical applications.
2. Consider the societal and lifecycle impacts of the solution.

#### **Origins of concern**

##### **Withholding Risks**
- **Unstated Biases**: LLM outputs often inherit biases from training data. Teams may fail to disclose these biases, leading users to believe the outputs are impartial or universally applicable.
- **Unexplained Edge Cases**: LLMs can produce unreliable or nonsensical outputs in specific scenarios, which might be overlooked or hidden in project demonstrations.
- **Omitted Ethical Concerns**: Participants might not discuss potential ethical risks, such as misuse of the LLM for malicious purposes or the generation of harmful or misleading content.

##### **Misleading Users About Reliability**
- **Overconfidence in Outputs**: Presenting LLM responses as definitive or authoritative, without clarifying their probabilistic nature, may lead users to overly trust the outputs.
- **Hiding Model Limitations**: Teams may avoid discussing issues like hallucinations (confidently generating false information), reliance on incomplete datasets, or lack of domain-specific expertise.
- **Failure to Acknowledge Developmental Stage**: Projects may present prototype-level solutions as production-ready, glossing over the immaturity or incompleteness of the system.

#### **Mitigation Strategies**

##### **Addressing Withheld Risks**
1. **Explicitly Acknowledge Biases**:
   - Document and discuss potential biases inherent in the LLM’s training data, especially those relevant to the project domain.
   - Share examples of biased or problematic outputs observed during development to raise awareness.
2. **Include Ethical Risk Disclosures**:
   - Outline potential misuse scenarios and any safeguards implemented to mitigate such risks.
   - Highlight unresolved ethical concerns and suggest areas for future work to address them.
3. **Test and Report Edge Cases**:
   - Actively test the LLM in diverse scenarios to identify edge cases where outputs may be unreliable.
   - Share findings transparently, emphasizing known failure points or limitations.

##### **Improving Transparency About Reliability**
1. **Use Disclaimers**:
   - Add clear disclaimers in presentations, documentation, and user interfaces, indicating that LLM outputs may be incomplete, biased, or inaccurate.
   - Example disclaimer: *“This tool is a prototype relying on probabilistic models and may produce inaccurate or unexpected outputs.”*
2. **Focus on Probabilistic Nature**:
   - Emphasize that LLMs generate outputs based on likelihoods rather than deterministic logic, and results should be verified in high-stakes applications.
3. **Clarify Prototype Status**:
   - Transparently communicate the current development stage of the solution, including areas requiring further testing or refinement.
   - Frame limitations as opportunities for future work, demonstrating awareness and a plan for addressing them.

